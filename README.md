# local-llm

Deploy a local LLM inference engine using llama.cpp

Follow the instructions on https://github.com/ggerganov/llama.cpp/tree/master to run llama-server. Once the model is running on the server, run llama2.py and talk to your local LLM!

- Future updates include a GUI to interact witht the LLM and a way to switch between different models.
