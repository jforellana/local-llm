# local-llm
Deploy a local LLM inference engine using llama.cpp
